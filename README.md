# PCA Implementation on Fashion MNIST Dataset ğŸ›ï¸ğŸ”¢

This project showcases the implementation of Principal Component Analysis (PCA) on the Fashion MNIST dataset. The goal is to demonstrate the use of PCA for dimensionality reduction and its impact on model performance.

## What is PCA?
Principal Component Analysis (PCA) is a popular dimensionality reduction technique used in machine learning and data analysis. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important information. PCA achieves this by identifying orthogonal axes, called principal components, that capture the maximum variance in the data.

# Project Details â„¹ï¸

## Dataset ğŸ“Š

The Fashion MNIST dataset is used in this project, which consists of 48,000 training samples with 784 features each. Each sample represents a 28x28 grayscale image of a fashion item from 10 different classes.

## Model and Baseline Accuracy ğŸ§ªğŸ“ˆ

Initially, the K Nearest Neighbors (KNN) algorithm from the scikit-learn library is applied to the dataset without any dimensionality reduction technique. This serves as the baseline accuracy for comparison. The KNN model achieved an accuracy of 0.85 on the Fashion MNIST dataset.

## Feature Scaling âš–ï¸

Before applying PCA, feature scaling is performed to standardize the features and bring them to a similar scale. This step ensures that each feature contributes equally during the PCA process.

## PCA Implementation ğŸ”„

PCA is then applied to the preprocessed dataset. The number of principal components chosen for this project is 50. These components are selected to retain the maximum amount of variance while significantly reducing the feature space.

## Impact of PCA on Accuracy and Computation âš¡ï¸ğŸ”

After implementing PCA with 50 principal components, the KNN model is trained and evaluated on the transformed dataset. Surprisingly, the accuracy remains the same at 0.85. However, the key advantage is a reduction in computational time and the number of features required for the model.

PCA offers several benefits, including:
Dimensionality reduction: By reducing the number of features, PCA simplifies the dataset, making it easier to analyze and visualize.
Computation efficiency: PCA allows for faster computation, especially when dealing with large datasets, by reducing the dimensionality.
Noise reduction: PCA can help filter out noise and focus on the most important features in the data.
Feature interpretation: Principal components can provide insights into the most influential features in the dataset.
Overall, this project demonstrates the application of PCA for dimensionality reduction and its impact on model performance. While the accuracy remains the same, PCA reduces computation time and feature space, making it an efficient technique for handling high-dimensional data.

Feel free to explore the code and experiment with different parameters and algorithms!
